{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shared\n",
    "import static_individual_estimates\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ve_or_ju = \"ve\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "startTime = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_orig = pd.read_csv(f'data/runs_{ve_or_ju}.tsv', delimiter=\"\\t\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_counts = runs_orig[\"team_country\"].value_counts()\n",
    "top_country_counts = country_counts[country_counts > 100]\n",
    "top_countries = top_country_counts.keys().tolist()\n",
    "display(top_countries)\n",
    "\n",
    "with open(f\"data/top_countries_{ve_or_ju}.json\", 'w') as outfile:\n",
    "    json.dump(top_countries, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_orig[\"first_name\"] = runs_orig.name.str.split(\" \", expand=True).iloc[:, 0]\n",
    "runs_df = runs_orig\n",
    "runs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_counts = runs_df[\"first_name\"].value_counts()\n",
    "top_fn_counts = fn_counts[fn_counts > 20]\n",
    "top_first_names = top_fn_counts.keys().tolist()\n",
    "\n",
    "with open(f\"data/top_first_names_{ve_or_ju}.json\", 'w') as outfile:\n",
    "    json.dump(top_first_names, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporarily remove 2018 in order to try predict it in other notebook\n",
    "#runs_df = runs_df[runs_df.year != 2018]\n",
    "#runs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = static_individual_estimates.preprocess_features(runs_df, top_countries, ve_or_ju)\n",
    "features.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = features.values\n",
    "#x = features[[\"team_id\", \"team_id_log10\", \"team_id_log100\", \"team_id_log2\", \"team_id_square\", \"leg_id_1\", \"leg_id_2\", \"leg_id_3\", \"leg_id_4\", \"leg_id_5\", \"leg_id_6\", \"leg_id_7\"]].values # Poista tää.\n",
    "y = np.log(runs_df.pace.values)\n",
    "y = y.reshape(len(y), 1)\n",
    "\n",
    "display(x.shape)\n",
    "display(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reports = [f'x.shape: {x.shape}', f'y.shape: {y.shape}', features.info()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn import linear_model\n",
    "from sklearn import ensemble\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=2019)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def fit_and_test_model(model, x_train, x_test, y_train, y_test, fit_params={}):\n",
    "    model.fit(x_train, y_train.ravel(), **fit_params)\n",
    "    y_pred = np.exp(model.predict(x_test))\n",
    "    print(f\"Shapes: y_test={np.exp(y_test).shape} y_pred={y_pred.shape}\")\n",
    "    print(\"Mean squared error: %.3f\" % mean_squared_error(np.exp(y_test), y_pred))\n",
    "    print('Explained variance score: %.3f' % r2_score(np.exp(y_test), y_pred))\n",
    "\n",
    "    reports.append(f'{type(model)}: {model.get_params()}')\n",
    "    reports.append(f'Explained variance score: {r2_score(np.exp(y_test), y_pred).round(3)}')\n",
    "    \n",
    "    plt.scatter(x_test[:,0], np.exp(y_test),  color='red', alpha=0.01)\n",
    "    plt.scatter(x_test[:,0], y_pred, color='blue', alpha=0.01)\n",
    "    plt.ylim(4, 20)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "linear = linear_model.LinearRegression()\n",
    "fit_and_test_model(linear, x_train, x_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'name':features.keys(), 'coef':linear.coef_})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "gbr_num_estimators=1000\n",
    "gbr = sklearn.ensemble.GradientBoostingRegressor(n_estimators=gbr_num_estimators, random_state=0, verbose=1)\n",
    "\n",
    "fit_and_test_model(gbr, x_train, x_test, y_train, y_test)\n",
    "\n",
    "#print(f\"feature_importances_: {gbr.feature_importances_}\")\n",
    "#gbr_features = pd.DataFrame({'feature':first_names.columns, 'importance': gbr.feature_importances_})\n",
    "#gbr_features['feature'] = gbr_features['feature'].str.replace('top_first_name_','')\n",
    "#display(gbr_features.sort_values(by=\"importance\", ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr_num_estimators_quantile=int(gbr_num_estimators/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr_q_low = sklearn.ensemble.GradientBoostingRegressor(loss='quantile', alpha=0.159, n_estimators=gbr_num_estimators_quantile, random_state=0, verbose=1)\n",
    "fit_and_test_model(gbr_q_low, x_train, x_test, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr_q_high = sklearn.ensemble.GradientBoostingRegressor(loss='quantile', alpha=0.841, n_estimators=gbr_num_estimators_quantile, random_state=0, verbose=1)\n",
    "fit_and_test_model(gbr_q_high, x_train, x_test, y_train, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(gbr, f'gbr_{ve_or_ju}.sav')\n",
    "joblib.dump(gbr_q_low, f'gbr_q_low_{ve_or_ju}.sav')\n",
    "joblib.dump(gbr_q_high, f'gbr_q_high_{ve_or_ju}.sav')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr_preds = gbr.predict(pd.DataFrame(x_test))\n",
    "gbr_q_low_preds = gbr_q_low.predict(pd.DataFrame(x_test))\n",
    "gbr_q_high_preds = gbr_q_high.predict(pd.DataFrame(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr_q_pred_errors = pd.DataFrame({\n",
    "    'q_low':np.exp(gbr_q_low_preds),\n",
    "    'true':np.exp(y_test).ravel(), \n",
    "    'predicted':np.exp(gbr_preds), \n",
    "    'q_high':np.exp(gbr_q_high_preds), \n",
    "})\n",
    "\n",
    "gbr_q_pred_errors[\"q_low_error\"] = gbr_q_pred_errors.true < gbr_q_pred_errors.q_low\n",
    "gbr_q_pred_errors[\"q_high_error\"] = gbr_q_pred_errors.true > gbr_q_pred_errors.q_high\n",
    "gbr_q_pred_errors[\"q_error\"] = np.logical_or(gbr_q_pred_errors.q_low_error, gbr_q_pred_errors.q_high_error)\n",
    "gbr_q_pred_errors[\"q_interval\"] = gbr_q_pred_errors.q_high - gbr_q_pred_errors.q_low\n",
    "\n",
    "\n",
    "gbr_q_pred_errors[\"std\"] = (gbr_q_pred_errors.q_high - gbr_q_pred_errors.q_low) / 2\n",
    "# Intentionally don't use log scale for calculation to get bigger std\n",
    "# TODO IS this causing big std in Bayesian models? :(\n",
    "gbr_q_pred_errors[\"std_correct\"] = np.exp((gbr_q_high_preds - gbr_q_low_preds) / 2)\n",
    "gbr_q_pred_errors[\"abs_error\"] = np.abs(gbr_q_pred_errors.predicted - gbr_q_pred_errors.true)\n",
    "gbr_q_pred_errors[\"abs_error_in_stds\"] = gbr_q_pred_errors.abs_error / np.exp(gbr_q_pred_errors[\"std_correct\"])\n",
    "\n",
    "gbr_reports = [\n",
    "    f'q_low_error.mean {gbr_q_pred_errors.q_low_error.mean().round(4)}',\n",
    "    f'q_high_error.mean {gbr_q_pred_errors.q_high_error.mean().round(4)}',\n",
    "    f'q_error.mean {gbr_q_pred_errors.q_error.mean().round(4)}',\n",
    "\n",
    "    f'q_interval.mean {gbr_q_pred_errors.q_interval.mean().round(4)}',\n",
    "    f'q_interval.median {gbr_q_pred_errors.q_interval.median().round(4)}',\n",
    "\n",
    "    f'std.mean {gbr_q_pred_errors[\"std\"].mean().round(4)}',\n",
    "    f'std_correct.mean {gbr_q_pred_errors[\"std_correct\"].mean().round(4)}',\n",
    "    f'abs_error_in_stds.mean {gbr_q_pred_errors[\"abs_error_in_stds\"].mean().round(4)}',\n",
    "    f'abs_error.mean {gbr_q_pred_errors[\"abs_error\"].mean().round(4)}',\n",
    "    f'abs_error.median {gbr_q_pred_errors[\"abs_error\"].median().round(4)}'\n",
    "]\n",
    "\n",
    "\n",
    "display(gbr_q_pred_errors.tail(15).round(3))\n",
    "display(gbr_reports)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reports.extend(gbr_reports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr_q_pred_errors[['q_error', \"q_low\", \"q_high\", \"q_interval\", \"abs_error\", \"std\"]].groupby('q_error').agg([\"median\"]).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endTime = time.time()\n",
    "reports.append(f\"runtime {round(((endTime - startTime)/ 60), 2)} mins\")\n",
    "shared.write_simple_text_report(reports, f'preprocess_priors_{ve_or_ju}.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared.log_df(f\"{ve_or_ju} runtime {round(((endTime - startTime)/ 60), 2)} mins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ['MKL_THREADING_LAYER'] = 'GNU'\n",
    "os.environ['THEANO_FLAGS'] = 'device=cpu'\n",
    "#os.environ['THEANO_FLAGS'] = 'device=cuda,floatX=float32,force_device=True'\n",
    "\n",
    "import pymc3 as pm\n",
    "import pmlearn\n",
    "from pmlearn.linear_model import LinearRegression\n",
    "print('Running on pymc-learn v{}'.format(pmlearn.__version__))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "pmlearn_linear = LinearRegression()\n",
    "fit_params={\n",
    "    \"inference_type\": \"nuts\",\n",
    "    \"inference_args\": {\n",
    "        \"cores\": multiprocessing.cpu_count() -1,\n",
    "        #\"chains\":2,\n",
    "        \"init\": 'adapt_diag',\n",
    "        #\"tune\": 2000,\n",
    "        \"target_accept\": 0.9999\n",
    "    }\n",
    "}\n",
    "fit_and_test_model(pmlearn_linear, pd.DataFrame(data=x_train), pd.DataFrame(x_test), y_train, y_test,fit_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmlearn_preds = pmlearn_linear.predict(pd.DataFrame(x_test), return_std=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "joblib.dump(pmlearn_linear, 'pmlearn_linear.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_errors = pd.DataFrame({\n",
    "    'mean':np.exp(pmlearn_preds[0]), \n",
    "    'std':np.exp(pmlearn_preds[1]), \n",
    "    'true':np.exp(y_test).ravel(), \n",
    "    'error':np.abs(np.exp(y_test).ravel() -np.exp(pmlearn_preds[0])) / np.exp(pmlearn_preds[1])\n",
    "})\n",
    "display(pred_errors.head(15))\n",
    "pred_errors.error.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmlearn_linear.plot_elbo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.traceplot(pmlearn_linear.trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.forestplot(pmlearn_linear.trace, varnames=[\"betas\", \"alpha\", \"s\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df = pm.summary(pmlearn_linear.trace, varnames=[\"betas\", \"alpha\", \"s\"])\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.plot_posterior(pmlearn_linear.trace, varnames=[\"betas\", \"alpha\", \"s\"],\n",
    "                 figsize = [14, 8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.gelman_rubin(pmlearn_linear.trace, varnames=[\"betas\", \"alpha\", \"s\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
