{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aca7104",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "import shared\n",
    "import os\n",
    "from scipy import stats\n",
    "\n",
    "os.environ['RACE_TYPE'] = \"ju\"\n",
    "os.environ['FORECAST_YEAR'] = \"2024\"\n",
    "race_type = shared.race_type()\n",
    "#year = shared.forecast_year()\n",
    "import time\n",
    "startTime = time.time()\n",
    "sns.set(rc={\"figure.figsize\":(16, 9)}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc09f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get all years use next year\n",
    "runs = pd.read_csv(f'data/runs_{shared.race_id_str()}.tsv', delimiter='\\t')\n",
    "#runs = runs.query(\"num_runs >= 1\")\n",
    "#runs = runs[\"pace\"].notna()\n",
    "runs = runs.dropna(subset=['pace'])\n",
    "runs[\"log_pace\"] = np.log(runs[\"pace\"])\n",
    "runs[\"very_slow\"] = runs[\"pace\"] > 30\n",
    "#runs[\"pace\"] = runs[\"pace\"].clip(upper=30)\n",
    "runs[\"num_runs\"] = runs[\"num_runs\"].clip(upper=14)\n",
    "runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5163267",
   "metadata": {},
   "outputs": [],
   "source": [
    "runs[\"pace\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68ca4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "runs[\"very_slow\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586880a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "1/runs[\"very_slow\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77e96d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "runs.sort_values([\"pace\"]).tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372b65e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_years = runs.year.unique()\n",
    "all_years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70089b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def satterthwaite_df(column):\n",
    "    \"\"\"Calculate the Satterthwaite degrees of freedom for a pandas column.\"\"\"\n",
    "    s = column.std()  # sample standard deviation\n",
    "    n = len(column)  # sample size\n",
    "    \n",
    "    return (s**2 / n)**2 / (s**4 / ((n-1) * n**2))\n",
    "\n",
    "\n",
    "\n",
    "def _student_t_params(column): \n",
    "    # Calculate the sample mean and sample variance\n",
    "    xbar = column.mean()\n",
    "    std = column.std()\n",
    "\n",
    "    # Define the log-likelihood function\n",
    "    def log_likelihood(nu):\n",
    "        return sum(stats.t.logpdf(column, df=nu, loc=xbar, scale=std))\n",
    "\n",
    "    # Find the maximum likelihood estimate of the degrees of freedom\n",
    "    nu_hat = pd.Series(range(1, 500)).apply(log_likelihood).idxmax() + 1\n",
    "\n",
    "    satterthwaite = satterthwaite_df(column)\n",
    "\n",
    "\n",
    "    return xbar, std, nu_hat, satterthwaite\n",
    "\n",
    "\n",
    "for year in all_years:\n",
    "    #print(year)\n",
    "    year_runs = runs[runs[\"year\"] == year]\n",
    "    #xbar, std, nu_hat, satterthwaite = _student_t_params(year_runs[\"log_pace\"])\n",
    "    # Print the results\n",
    "    #print(f'{year} Sample mean: {xbar:.2f}, Sample std: {std:.2f}, Degrees of freedom: {nu_hat}, satterthwaite: {satterthwaite:.1f}, min: {year_runs[\"pace\"].min():.1f}, max: {year_runs[\"pace\"].max():.1f}')\n",
    "    # Fit a t-distribution to your data\n",
    "    degrees_of_freedom, location, scale = stats.t.fit(year_runs[\"log_pace\"])\n",
    "    print(f'{year} St fit medi: {location:.2f}, Sample std: {scale:.2f}, degrees_of_freedom: {degrees_of_freedom:.1f}, min: {year_runs[\"pace\"].min():.1f}, max: {year_runs[\"pace\"].max():.1f}')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb35fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees_of_freedom, location, scale = stats.t.fit(runs[\"log_pace\"])\n",
    "# Simulate the same amount of random data with the estimated parameters\n",
    "simulated_data = stats.t.rvs(degrees_of_freedom, location, scale, size=len(runs[\"log_pace\"]))\n",
    "\n",
    "print(f'ALL ST fit medi: {location:.2f}, Sample std: {scale:.2f}, degrees_of_freedom: {degrees_of_freedom:.1f}, min: {sub_runs[\"pace\"].min():.1f}, max: {sub_runs[\"pace\"].max():.1f}')\n",
    "fig, ax = plt.subplots()\n",
    "sns.histplot(runs[\"log_pace\"], bins=100, color='blue', kde=True, label='Original Data', alpha=0.4, ax=ax)\n",
    "sns.histplot(simulated_data, bins=100, color='red', kde=True, label='Simulated Data', alpha=0.4, ax=ax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12a5cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees_of_freedom, location, scale = stats.t.fit(runs[\"log_pace\"])\n",
    "# Simulate the same amount of random data with the estimated parameters\n",
    "simulated_data_2 = stats.t.rvs(degrees_of_freedom, location, scale, size=len(runs[\"log_pace\"]))\n",
    "\n",
    "# Assuming runs is a dictionary or a pandas DataFrame with a \"log_pace\" key/column\n",
    "min_value = np.min(runs[\"log_pace\"]) + 0.1\n",
    "\n",
    "# Filter values in simulated_data that are below the minimum of runs[\"log_pace\"]\n",
    "below_min_values = simulated_data_2[simulated_data_2 < min_value]\n",
    "\n",
    "# Resample these values\n",
    "resampled_values = np.random.choice(simulated_data_2, size=len(below_min_values), replace=True)\n",
    "\n",
    "# Replace the below-minimum values in simulated_data with the resampled values\n",
    "simulated_data_2[simulated_data_2 < min_value] = resampled_values\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "sns.histplot(runs[\"log_pace\"], bins=100, color='blue', kde=True, label='Original Data', alpha=0.4, ax=ax)\n",
    "sns.histplot(simulated_data_2, bins=100, color='red', kde=True, label='Simulated Data', alpha=0.4, ax=ax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391e53ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees_of_freedom, location, scale = stats.t.fit(runs[\"log_pace\"])\n",
    "simulated_data_3 = stats.t.rvs(degrees_of_freedom, location, scale, size=len(runs[\"log_pace\"]))\n",
    "# Define the threshold for the tail\n",
    "threshold = np.percentile(simulated_data_3, 85)\n",
    "\n",
    "# Fit the Pareto distribution to the upper tail\n",
    "shape_param, loc, scale_param = stats.pareto.fit(simulated_data_3[simulated_data_3 > threshold])\n",
    "\n",
    "num_tail_values = sum(simulated_data_3 > threshold)\n",
    "new_tail_data = stats.pareto.rvs(shape_param, loc, scale_param, size=num_tail_values)\n",
    "\n",
    "simulated_data_3[simulated_data_3 > threshold] = new_tail_data\n",
    "\n",
    "\n",
    "\n",
    "#shape_param, loc, scale_param = stats.pareto.fit(runs[\"log_pace\"])\n",
    "# Simulate the same amount of random data with the estimated parameters\n",
    "#pareto_simulated_data = stats.pareto.rvs(shape_param, loc, scale_param, size=len(runs[\"log_pace\"]))\n",
    "\n",
    "#print(f'ALL ST fit medi: {location:.2f}, Sample std: {scale:.2f}, degrees_of_freedom: {degrees_of_freedom:.1f}, min: {sub_runs[\"pace\"].min():.1f}, max: {sub_runs[\"pace\"].max():.1f}')\n",
    "fig, ax = plt.subplots()\n",
    "sns.histplot(runs[\"log_pace\"], bins=100, color='blue', kde=True, label='Original Data', alpha=0.4, ax=ax)\n",
    "sns.histplot(simulated_data_3, bins=100, color='red', kde=True, label='Simulated Data', alpha=0.4, ax=ax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7646aa38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_weighted_t(data, weight_power=2):\n",
    "    # Fit data to Student's t distribution\n",
    "    df, loc, scale = stats.t.fit(data)\n",
    "    \n",
    "    # Simulate data from the estimated parameters\n",
    "    simulated_data = stats.t.rvs(df, loc, scale, size=len(data))\n",
    "    \n",
    "    # Assign weights to simulated data\n",
    "    # Here, we're raising each sample to a power to emphasize the right tail\n",
    "    # The weight_power determines how much emphasis to put on the right tail\n",
    "    weights = simulated_data ** weight_power\n",
    "    \n",
    "    # Normalize weights to make them sum to 1\n",
    "    weights /= weights.sum()\n",
    "    \n",
    "    # Resample from the simulated data using the weights\n",
    "    weighted_samples = np.random.choice(simulated_data, size=len(data), p=weights)\n",
    "    \n",
    "    return weighted_samples\n",
    "\n",
    "weighted_simulated = simulate_weighted_t(runs[\"log_pace\"], weight_power=1.7)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "sns.histplot(runs[\"log_pace\"], bins=30, color='blue', kde=True, label='Original Data', alpha=0.4, ax=ax)\n",
    "sns.histplot(weighted_simulated, bins=30, color='red', kde=True, label='Simulated Data', alpha=0.4, ax=ax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58192a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "def get_kde_samples(data, n_samples=None):\n",
    "    \"\"\"Get samples from KDE.\"\"\"\n",
    "    if n_samples is None:\n",
    "        n_samples = len(data)\n",
    "        \n",
    "    kde = gaussian_kde(data)\n",
    "    print(kde)\n",
    "    # Setting bounds for the sampled data\n",
    "    min_val, max_val = min(data), max(data)\n",
    "    simulated_data = kde.resample(n_samples).flatten()\n",
    "    \n",
    "    # If the sampled data goes beyond original bounds, we resample those points\n",
    "    while any(simulated_data < min_val) or any(simulated_data > max_val):\n",
    "        out_of_bounds = (simulated_data < min_val) | (simulated_data > max_val)\n",
    "        new_samples = kde.resample(out_of_bounds.sum()).flatten()\n",
    "        simulated_data[out_of_bounds] = new_samples\n",
    "\n",
    "    return simulated_data\n",
    "\n",
    "\n",
    "simulated_data = get_kde_samples(runs[\"log_pace\"])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "sns.histplot(runs[\"log_pace\"], bins=30, color='blue', kde=True, label='Original Data', alpha=0.4, ax=ax)\n",
    "sns.histplot(simulated_data, bins=30, color='red', kde=True, label='Simulated Data', alpha=0.4, ax=ax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fd71c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_simulate(data, dist_name):\n",
    "    if dist_name == 'gamma':\n",
    "        params = stats.gamma.fit(data)\n",
    "        simulated_data = stats.gamma.rvs(*params, size=len(data))\n",
    "    elif dist_name == 'weibull':\n",
    "        params = stats.weibull_min.fit(data)  # Note: we're using weibull_min which represents the common 3-parameter Weibull\n",
    "        simulated_data = stats.weibull_min.rvs(*params, size=len(data))\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown distribution: {dist_name}\")\n",
    "    \n",
    "    return simulated_data, params\n",
    "\n",
    "def plot_data_and_simulated(data, simulated, dist_name):\n",
    "    #plt.figure(figsize=(15, 6))\n",
    "    \n",
    "    sns.histplot(data, kde=True, color=\"blue\", label=\"Original Data\", alpha=0.5)\n",
    "    sns.histplot(weibull_simulated, kde=True, color=\"red\", label=f\"{dist_name} Simulated\", alpha=0.5)\n",
    "    \n",
    "    plt.title(f'Original vs {dist_name}Â Simulated Data')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Sample usage:\n",
    "data = runs[\"log_pace\"]  # Replace with your data\n",
    "\n",
    "gamma_simulated, gamma_params = fit_and_simulate(data, 'gamma')\n",
    "weibull_simulated, weibull_params = fit_and_simulate(data, 'weibull')\n",
    "\n",
    "plot_data_and_simulated(data, gamma_simulated, \"Gamma\")\n",
    "plot_data_and_simulated(data, weibull_simulated, \"weibull\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b120e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6877a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_params = []\n",
    "for num_runs, sub_runs in runs.groupby(\"num_runs\"):\n",
    "    #display(sub_runs)\n",
    "    #num_runs = sub_runs[\"num_runs\"].unique()\n",
    "    #xbar, std, nu_hat, satterthwaite = _student_t_params(sub_runs[\"log_pace\"])\n",
    "    # Print the results\n",
    "    #print(f'{num_runs} Sample mean: {xbar:.2f}, Sample std: {std:.2f}, Degrees of freedom: {nu_hat}, min: {sub_runs[\"pace\"].min():.1f}, max: {sub_runs[\"pace\"].max():.1f}')\n",
    "    degrees_of_freedom, location, scale = stats.t.fit(sub_runs[\"log_pace\"])\n",
    "    # Simulate the same amount of random data with the estimated parameters\n",
    "    simulated_data = stats.t.rvs(degrees_of_freedom, location, scale, size=len(sub_runs[\"log_pace\"]))\n",
    "\n",
    "    #print(f'{num_runs} ST fit medi: {location:.2f}, Sample std: {scale:.2f}, degrees_of_freedom: {degrees_of_freedom:.1f}, min: {sub_runs[\"pace\"].min():.1f}, max: {sub_runs[\"pace\"].max():.1f}')\n",
    "    all_params.append({\n",
    "        \"num_runs\": num_runs,\n",
    "        \"dof\": degrees_of_freedom,\n",
    "        \"location\": location,\n",
    "        \"scale\": scale\n",
    "    })\n",
    "\n",
    "    # Fit a t-distribution to your data\n",
    "    # Calculate robust estimates of the location and scale\n",
    "    #location = sub_runs[\"log_pace\"].median()\n",
    "    #scale = stats.median_abs_deviation(sub_runs[\"log_pace\"])\n",
    "\n",
    "    # Fit a t-distribution to your data, fixing the location and scale\n",
    "    #degrees_of_freedom, _, _ = stats.t.fit(sub_runs[\"log_pace\"], floc=location, fscale=scale)\n",
    "    #simulated_data_2 = stats.t.rvs(degrees_of_freedom, location, scale, size=len(sub_runs[\"log_pace\"]))\n",
    "\n",
    "    #print(f'{num_runs} ST fit medi: {location:.2f}, Sample std: {scale:.2f}, degrees_of_freedom: {degrees_of_freedom:.1f}, min: {sub_runs[\"pace\"].min():.1f}, max: {sub_runs[\"pace\"].max():.1f}')\n",
    "    fig, ax = plt.subplots()\n",
    "    sns.histplot(sub_runs[\"log_pace\"], bins=30, color='blue', kde=True, label='Original Data', alpha=0.4, ax=ax)\n",
    "    sns.histplot(simulated_data, bins=30, color='red', kde=True, label='Simulated Data', alpha=0.4, ax=ax)\n",
    "    ax.set_title(f'num_runs = {num_runs}, n = {len(sub_runs)}, dof = {degrees_of_freedom:.1f}, std = {scale:.2f}')\n",
    "    ax.legend()\n",
    "    #sns.histplot(simulated_data_2, bins=30, color='green', kde=True, label='Simulated Data', alpha=0.4, ax=ax)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a346b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.json_normalize(all_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d782ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(x=\"log_pace\", hue=\"num_runs\", kind=\"kde\", height=6, aspect=1.7, palette=\"bright\", data=runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6943f794",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf61a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a t-distribution to the data in the specified column\n",
    "df, loc, scale = stats.t.fit(runs[\"log_pace\"])\n",
    "\n",
    "# Calculate the CDF values of the fitted t-distribution for each data point\n",
    "cdf_values = stats.t.cdf(runs[\"log_pace\"], df, loc, scale)\n",
    "\n",
    "# Perform the K-S test\n",
    "test_statistic, p_value = stats.kstest(runs[\"log_pace\"], cdf_values)\n",
    "\n",
    "# Print the results\n",
    "print(f'Test statistic: {test_statistic}')\n",
    "print(f'p-value: {p_value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d473471",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
