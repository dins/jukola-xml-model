{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shared\n",
    "import static_individual_estimates\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ve_or_ju = \"ju\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "startTime = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = pd.read_csv(f'data/grouped_paces_{ve_or_ju}.tsv', delimiter=\"\\t\")\n",
    "grouped.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped[\"leg_nro\"] = grouped[\"most_common_leg\"]\n",
    "grouped[\"team_id\"] = grouped[\"mean_team_id\"]\n",
    "grouped[\"team_country\"] = grouped[\"most_common_country\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_counts = grouped[\"team_country\"].value_counts()\n",
    "top_country_counts = country_counts[country_counts > 30]\n",
    "top_countries = top_country_counts.keys().tolist()\n",
    "display(top_countries)\n",
    "\n",
    "with open(f\"data/top_countries_{ve_or_ju}.json\", 'w') as outfile:\n",
    "    json.dump(top_countries, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = grouped[grouped[\"mean_pace\"].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped[\"first_name\"] = grouped.name.str.split(\" \", expand=True).iloc[:, 0]\n",
    "grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_counts = grouped[\"first_name\"].value_counts()\n",
    "top_fn_counts = fn_counts[fn_counts > 10]\n",
    "top_first_names = top_fn_counts.keys().tolist()\n",
    "\n",
    "with open(f\"data/top_first_names_{ve_or_ju}.json\", 'w') as outfile:\n",
    "    json.dump(top_first_names, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporarily remove 2018 in order to try predict it in other notebook\n",
    "#runs_df = runs_df[runs_df.year != 2018]\n",
    "#runs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = static_individual_estimates.preprocess_features(grouped, top_countries, ve_or_ju)\n",
    "features.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = features.values\n",
    "y = np.log(grouped.mean_pace.values)\n",
    "y = y.reshape(len(y), 1)\n",
    "\n",
    "display(x.shape)\n",
    "display(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reports = [f'x.shape: {x.shape}', f'y.shape: {y.shape}', features.info()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn import linear_model\n",
    "from sklearn import ensemble\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=2019)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def fit_and_test_model(model, x_train, x_test, y_train, y_test, fit_params={}):\n",
    "    model.fit(x_train, y_train.ravel(), **fit_params)\n",
    "    y_pred = np.exp(model.predict(x_test))\n",
    "    print(f\"Shapes: y_test={np.exp(y_test).shape} y_pred={y_pred.shape}\")\n",
    "    print(\"Mean squared error: %.3f\" % mean_squared_error(np.exp(y_test), y_pred))\n",
    "    print('Explained variance score: %.3f' % r2_score(np.exp(y_test), y_pred))\n",
    "\n",
    "    reports.append(f'{type(model)}: {model.get_params()}')\n",
    "    reports.append(f'Explained variance score: {r2_score(np.exp(y_test), y_pred).round(3)}')\n",
    "    \n",
    "    plt.scatter(x_test[:,0], np.exp(y_test),  color='red', alpha=0.01)\n",
    "    plt.scatter(x_test[:,0], y_pred, color='blue', alpha=0.01)\n",
    "    plt.ylim(4, 20)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "linear = linear_model.LinearRegression()\n",
    "fit_and_test_model(linear, x_train, x_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = pd.DataFrame({'name':features.keys(), 'coef':linear.coef_})\n",
    "display(coefs.sort_values(by=\"coef\").round(4))\n",
    "#display(coefs.sort_values(by=\"coef\", ascending=False).head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "gbr_num_estimators=1000\n",
    "gbr = sklearn.ensemble.GradientBoostingRegressor(n_estimators=gbr_num_estimators, random_state=0, verbose=1)\n",
    "\n",
    "fit_and_test_model(gbr, x_train, x_test, y_train, y_test)\n",
    "\n",
    "#print(f\"feature_importances_: {gbr.feature_importances_}\")\n",
    "#gbr_features = pd.DataFrame({'feature':first_names.columns, 'importance': gbr.feature_importances_})\n",
    "#gbr_features['feature'] = gbr_features['feature'].str.replace('top_first_name_','')\n",
    "#display(gbr_features.sort_values(by=\"importance\", ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr_num_estimators_quantile=int(gbr_num_estimators/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr_q_low = sklearn.ensemble.GradientBoostingRegressor(loss='quantile', alpha=0.159, n_estimators=gbr_num_estimators_quantile, random_state=0, verbose=1)\n",
    "fit_and_test_model(gbr_q_low, x_train, x_test, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr_q_high = sklearn.ensemble.GradientBoostingRegressor(loss='quantile', alpha=0.841, n_estimators=gbr_num_estimators_quantile, random_state=0, verbose=1)\n",
    "fit_and_test_model(gbr_q_high, x_train, x_test, y_train, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(gbr, f'gbr_{ve_or_ju}.sav')\n",
    "joblib.dump(gbr_q_low, f'gbr_q_low_{ve_or_ju}.sav')\n",
    "joblib.dump(gbr_q_high, f'gbr_q_high_{ve_or_ju}.sav')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr_preds = gbr.predict(pd.DataFrame(x_test))\n",
    "gbr_q_low_preds = gbr_q_low.predict(pd.DataFrame(x_test))\n",
    "gbr_q_high_preds = gbr_q_high.predict(pd.DataFrame(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr_q_pred_errors = pd.DataFrame({\n",
    "    'q_low':np.exp(gbr_q_low_preds),\n",
    "    'true':np.exp(y_test).ravel(), \n",
    "    'predicted':np.exp(gbr_preds), \n",
    "    'q_high':np.exp(gbr_q_high_preds), \n",
    "})\n",
    "\n",
    "gbr_q_pred_errors[\"q_low_error\"] = gbr_q_pred_errors.true < gbr_q_pred_errors.q_low\n",
    "gbr_q_pred_errors[\"q_high_error\"] = gbr_q_pred_errors.true > gbr_q_pred_errors.q_high\n",
    "gbr_q_pred_errors[\"q_error\"] = np.logical_or(gbr_q_pred_errors.q_low_error, gbr_q_pred_errors.q_high_error)\n",
    "gbr_q_pred_errors[\"q_interval\"] = gbr_q_pred_errors.q_high - gbr_q_pred_errors.q_low\n",
    "\n",
    "\n",
    "gbr_q_pred_errors[\"std\"] = (gbr_q_pred_errors.q_high - gbr_q_pred_errors.q_low) / 2\n",
    "# Intentionally don't use log scale for calculation to get bigger std\n",
    "# TODO IS this causing big std in Bayesian models? :(\n",
    "gbr_q_pred_errors[\"std_correct\"] = np.exp((gbr_q_high_preds - gbr_q_low_preds) / 2)\n",
    "gbr_q_pred_errors[\"abs_error\"] = np.abs(gbr_q_pred_errors.predicted - gbr_q_pred_errors.true)\n",
    "gbr_q_pred_errors[\"abs_error_in_stds\"] = gbr_q_pred_errors.abs_error / np.exp(gbr_q_pred_errors[\"std_correct\"])\n",
    "\n",
    "gbr_reports = [\n",
    "    f'q_low_error.mean {gbr_q_pred_errors.q_low_error.mean().round(4)}',\n",
    "    f'q_high_error.mean {gbr_q_pred_errors.q_high_error.mean().round(4)}',\n",
    "    f'q_error.mean {gbr_q_pred_errors.q_error.mean().round(4)}',\n",
    "\n",
    "    f'q_interval.mean {gbr_q_pred_errors.q_interval.mean().round(4)}',\n",
    "    f'q_interval.median {gbr_q_pred_errors.q_interval.median().round(4)}',\n",
    "\n",
    "    f'std.mean {gbr_q_pred_errors[\"std\"].mean().round(4)}',\n",
    "    f'std_correct.mean {gbr_q_pred_errors[\"std_correct\"].mean().round(4)}',\n",
    "    f'abs_error_in_stds.mean {gbr_q_pred_errors[\"abs_error_in_stds\"].mean().round(4)}',\n",
    "    f'abs_error.mean {gbr_q_pred_errors[\"abs_error\"].mean().round(4)}',\n",
    "    f'abs_error.median {gbr_q_pred_errors[\"abs_error\"].median().round(4)}'\n",
    "]\n",
    "\n",
    "\n",
    "display(gbr_q_pred_errors.tail(15).round(3))\n",
    "display(gbr_reports)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reports.extend(gbr_reports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr_q_pred_errors[['q_error', \"q_low\", \"q_high\", \"q_interval\", \"abs_error\", \"std\"]].groupby('q_error').agg([\"median\"]).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endTime = time.time()\n",
    "reports.append(f\"runtime {round(((endTime - startTime)/ 60), 2)} mins\")\n",
    "shared.write_simple_text_report(reports, f'preprocess_priors_grouped_{ve_or_ju}.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared.log_df(f\"{ve_or_ju} runtime {round(((endTime - startTime)/ 60), 2)} mins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
