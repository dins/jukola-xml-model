{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shared\n",
    "import static_individual_estimates\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ve_or_ju = shared.race_type()\n",
    "ve_or_ju"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "startTime = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x, y, features) = static_individual_estimates.preprocess_countries_names_and_features()\n",
    "features.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(x.shape)\n",
    "display(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reports = [f'x.shape: {x.shape}', f'y.shape: {y.shape}', features.info()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn import linear_model\n",
    "from sklearn import ensemble\n",
    "from sklearn.metrics import mean_squared_error, median_absolute_error, mean_absolute_percentage_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.05, random_state=2019)\n",
    "reports.append(f'x_train: {x_train.shape}, x_test: {x_test.shape}')\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "index_of_team_id = list(features.columns).index(\"team_id\")\n",
    "def fit_and_test_model(model, x_train, x_test, y_train, y_test, fit_params={}):\n",
    "    model.fit(x_train, y_train.ravel(), **fit_params)\n",
    "    y_pred = np.exp(model.predict(x_test))\n",
    "    print(f\"Shapes: y_test={np.exp(y_test).shape} y_pred={y_pred.shape}\")\n",
    "    print(\"Mean absolute percetange error: %.3f\" %  mean_absolute_percentage_error(np.exp(y_test), y_pred))\n",
    "    print(\"Median absolute error: %.3f\" %  median_absolute_error(np.exp(y_test), y_pred))\n",
    "    print(\"Mean squared error: %.3f\" % mean_squared_error(np.exp(y_test), y_pred))\n",
    "    print('Explained variance score: %.3f' % r2_score(np.exp(y_test), y_pred))\n",
    "\n",
    "    reports.append(f'{type(model)}: {model.get_params()}')\n",
    "    reports.append(f'Explained variance score: {r2_score(np.exp(y_test), y_pred).round(3)}')\n",
    "    \n",
    "    plt.scatter(x_test[:,index_of_team_id], np.exp(y_test),  color='red', alpha=0.01)\n",
    "    plt.scatter(x_test[:,index_of_team_id], y_pred, color='blue', alpha=0.01)\n",
    "    plt.ylim(4, 20)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "linear = linear_model.LinearRegression()\n",
    "fit_and_test_model(linear, x_train, x_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = pd.DataFrame({'name':features.keys(), 'coef':linear.coef_})\n",
    "display(coefs.sort_values(by=\"coef\").round(4))\n",
    "#display(coefs.sort_values(by=\"coef\", ascending=False).head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = linear_model.Ridge(alpha=0.5)\n",
    "fit_and_test_model(ridge, x_train, x_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"models/best_params_gbr_{shared.race_id_str()}.json\") as infile:\n",
    "    best_params = json.load(infile)\n",
    "\n",
    "best_params    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr = sklearn.ensemble.GradientBoostingRegressor(random_state=0, verbose=1, **best_params)\n",
    "reports.append(f\"GradientBoostingRegressor params: {gbr.get_params(deep=False)}\")\n",
    "\n",
    "fit_and_test_model(gbr, x_train, x_test, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Careful, impurity-based feature importances can be misleading for high cardinality features (many unique values). \n",
    "gbr_features = pd.DataFrame({'feature':features.columns, 'importance': gbr.feature_importances_})\n",
    "display(gbr_features.sort_values(by=\"importance\", ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "result = permutation_importance(gbr, x_test, y_test, n_repeats=20,\n",
    "                                random_state=2019, n_jobs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr_features['perm_importances_mean'] = result.importances_mean\n",
    "gbr_features['perm_importances_std'] = result.importances_std\n",
    "gbr_features['importance_power'] = np.sqrt(gbr_features['importance'] * gbr_features['perm_importances_mean'].abs())\n",
    "display(gbr_features.sort_values(by=\"perm_importances_mean\", ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr_q_low = sklearn.ensemble.GradientBoostingRegressor(loss='quantile', alpha=0.159, random_state=0, verbose=1, **best_params)\n",
    "fit_and_test_model(gbr_q_low, x_train, x_test, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr_q_high = sklearn.ensemble.GradientBoostingRegressor(loss='quantile', alpha=0.841, random_state=0, verbose=1, **best_params)\n",
    "fit_and_test_model(gbr_q_high, x_train, x_test, y_train, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "joblib.dump(gbr, f'models/gbr_{shared.race_id_str()}.sav')\n",
    "joblib.dump(gbr_q_low, f'models/gbr_q_low_{shared.race_id_str()}.sav')\n",
    "joblib.dump(gbr_q_high, f'models/gbr_q_high_{shared.race_id_str()}.sav')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr_preds = gbr.predict(pd.DataFrame(x_test))\n",
    "gbr_q_low_preds = gbr_q_low.predict(pd.DataFrame(x_test))\n",
    "gbr_q_high_preds = gbr_q_high.predict(pd.DataFrame(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr_q_pred_errors = pd.DataFrame({\n",
    "    'q_low':np.exp(gbr_q_low_preds),\n",
    "    'true':np.exp(y_test).ravel(), \n",
    "    'predicted':np.exp(gbr_preds), \n",
    "    'q_high':np.exp(gbr_q_high_preds), \n",
    "})\n",
    "\n",
    "gbr_q_pred_errors[\"q_low_error\"] = gbr_q_pred_errors.true < gbr_q_pred_errors.q_low\n",
    "gbr_q_pred_errors[\"q_high_error\"] = gbr_q_pred_errors.true > gbr_q_pred_errors.q_high\n",
    "gbr_q_pred_errors[\"q_error\"] = np.logical_or(gbr_q_pred_errors.q_low_error, gbr_q_pred_errors.q_high_error)\n",
    "gbr_q_pred_errors[\"q_interval\"] = gbr_q_pred_errors.q_high - gbr_q_pred_errors.q_low\n",
    "\n",
    "\n",
    "gbr_q_pred_errors[\"std\"] = (gbr_q_pred_errors.q_high - gbr_q_pred_errors.q_low) / 2\n",
    "# Intentionally don't use log scale for calculation to get bigger std\n",
    "# TODO IS this causing big std in Bayesian models? :(\n",
    "gbr_q_pred_errors[\"std_correct\"] = np.exp((gbr_q_high_preds - gbr_q_low_preds) / 2)\n",
    "gbr_q_pred_errors[\"abs_error\"] = np.abs(gbr_q_pred_errors.predicted - gbr_q_pred_errors.true)\n",
    "gbr_q_pred_errors[\"abs_error_in_stds\"] = gbr_q_pred_errors.abs_error / np.exp(gbr_q_pred_errors[\"std_correct\"])\n",
    "\n",
    "gbr_reports = [\n",
    "    f'q_low_error.mean {gbr_q_pred_errors.q_low_error.mean().round(4)}',\n",
    "    f'q_high_error.mean {gbr_q_pred_errors.q_high_error.mean().round(4)}',\n",
    "    f'q_error.mean {gbr_q_pred_errors.q_error.mean().round(4)}',\n",
    "\n",
    "    f'q_interval.mean {gbr_q_pred_errors.q_interval.mean().round(4)}',\n",
    "    f'q_interval.median {gbr_q_pred_errors.q_interval.median().round(4)}',\n",
    "\n",
    "    f'std.mean {gbr_q_pred_errors[\"std\"].mean().round(4)}',\n",
    "    f'std_correct.mean {gbr_q_pred_errors[\"std_correct\"].mean().round(4)}',\n",
    "    f'abs_error_in_stds.mean {gbr_q_pred_errors[\"abs_error_in_stds\"].mean().round(4)}',\n",
    "    f'abs_error.mean {gbr_q_pred_errors[\"abs_error\"].mean().round(4)}',\n",
    "    f'abs_error.median {gbr_q_pred_errors[\"abs_error\"].median().round(4)}'\n",
    "]\n",
    "\n",
    "\n",
    "display(gbr_q_pred_errors.tail(15).round(3))\n",
    "display(gbr_reports)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reports.extend(gbr_reports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr_q_pred_errors[['q_error', \"q_low\", \"q_high\", \"q_interval\", \"abs_error\", \"std\"]].groupby('q_error').agg([\"median\"]).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endTime = time.time()\n",
    "reports.append(f\"runtime {round(((endTime - startTime)/ 60), 2)} mins\")\n",
    "shared.write_simple_text_report(reports, f'preprocess_priors_grouped_{shared.race_id_str()}.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared.log_df(f\"{shared.race_id_str()} runtime {round(((endTime - startTime)/ 60), 2)} mins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
